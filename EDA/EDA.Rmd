---
title: "EDA"
author: "Alexis Laks"
date: "4 décembre 2019"
output: 
  html_document:
    toc : true
---

```{r setup, include=FALSE}
# Markdown formatting
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

```{r}
#Lodaing libraries
library(tidyverse)
library(dummies)
library(corrplot)
library(kableExtra)
library(broom)
library(lubridate)
library(feather)
```

# Data formatting

We have three datasets which we can concatenate by date or by date and identifier. The data sets are the following:

- fundamentals.csv : 
- crsp_top1000.csv : information on stock returns and other stock characteristics for each identifier at a given date.
- FF.csv : information on characteristics of the global market for each month of years 1990 to 2019. 

From these data we obtain a significant amount of variables and rows. We need to find focus point to avoid a chaotic analysis. A first lead would be to check for performance of certain portfolios across time for recent years. An indicator we can calculate and is missing from the data is the alpha or Jensen index which can be derived from the portfolio return formula:

r = Rf + beta * (Rm – Rf) + Alpha
<=> Alpha = r – Rf  – beta * (Rm – Rf)

where:

r = the security’s or portfolio’s return
Rf  = the risk-free rate of return
beta = systemic risk of a portfolio (the security’s or portfolio’s price volatility relative to the overall market)
Rm  = the market return

```{r}
# fundamentals data
ftls <- read.csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/fundamentals.csv", 
                 sep = "\t",
                 header = TRUE) %>% 
  mutate(day = substr(public_date,1,2),
         month = substr(public_date,3,5),
         year = substr(public_date,6,9)) %>% 
  mutate(month = case_when(month == "jan" ~ "1",
                           month == "feb" ~ "2",
                           month == "mar" ~ "3",
                           month == "apr" ~ "4",
                           month == "may" ~ "5",
                           month == "jun" ~ "6",
                           month == "jul" ~ "7",
                           month == "aug" ~ "8",
                           month == "sep" ~ "9",
                           month == "oct" ~ "10",
                           month == "nov" ~ "11",
                           month == "dec" ~ "12",
                           TRUE ~ "0")) %>% 
  select(-public_date)


# ff data

ff <- read.csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/FF.csv", 
                 sep = "\t",
                 header = TRUE) %>% 
  mutate(month = as.character(month),
         year = as.character(year))


# crsp top 100 data

crsp <- read.csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/crsp_top1000.csv", 
                 sep = "\t",
                 header = TRUE) %>% 
  mutate(month = substr(date,3,5),
         year = substr(date,6,9)) %>% 
  mutate(month = case_when(month == "jan" ~ "1",
                           month == "feb" ~ "2",
                           month == "mar" ~ "3",
                           month == "apr" ~ "4",
                           month == "may" ~ "5",
                           month == "jun" ~ "6",
                           month == "jul" ~ "7",
                           month == "aug" ~ "8",
                           month == "sep" ~ "9",
                           month == "oct" ~ "10",
                           month == "nov" ~ "11",
                           month == "dec" ~ "12",
                           TRUE ~ "0"))
```

We'll keep data only on portfolios for which we have data on the whole period we consider. To do this, we check which portfolios have the max number of dates in both fundamentals and crsp, we then retrieve the portfolio identifiers and keep only those in common. m

```{r}
valid_portfolios_ftls <- ftls %>% 
  group_by(lpermno) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n >= 348)

valid_portfolios_crsp <- crsp %>% 
  group_by(lpermno) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n >= 348)

valid_portfolios <- unique((valid_portfolios_crsp %>% 
  inner_join(valid_portfolios_ftls, 
             by = "lpermno"))$lpermno)
```

# ```{r}
# Just some checks to see how many portfolios we drop:
# length(valid_portfolios)
# 
# ftls %>%
#   group_by(lpermno) %>%
#   count() %>%
#   ungroup() %>%
#   group_by(n) %>%
#   count() %>%
#   filter(n <348) %>%
#   ungroup() %>%
#   summarize(sum(nn))
# ```

We drop a significant amount of portfolios, however this can be backed by the fact that lack of information in for some periods would result in biased results and in turn a biased analysis. 

After filtering the portfolios for which we have complete dara, we merge all the data in order to have all the information we potentially need going further in our analysis.

```{r}
crsp <- crsp %>% 
  filter(lpermno %in% valid_portfolios)

ftls <- ftls %>% 
  filter(lpermno %in% valid_portfolios)

merged_data <- crsp %>% 
    left_join(ftls,
              by = c("year","month","lpermno")) %>% 
    left_join(ff %>% 
                select(-date),
              by = c("year","month"))  %>% 
  drop_na(c(ret,rf,b_mkt,mktrf)) %>% 
  mutate(alpha = ret - rf - b_mkt * (mktrf - rf)) 

# Just creating a clean date format for future use in pyhton

merged_data <- merged_data %>% 
  mutate(date = with(merged_data, ymd(sprintf('%04s%02s%02s', year, month, day))))
```

Now that we've cleaned and formatted the data, we'll create two samples. The first one will be the in sample data which corresponds to portfolio data going from Jan 1990 to Dec 2015. The second one will be the out of sample data going from Jan 2016 till Dec 2019. We'll conduct our analysis and modelling on the in sample data and check our strategy's worth on the out of sample data.

```{r}
write_feather(merged_data %>% filter(year < 2016),
               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/in_sample_data.feather")

write_feather(merged_data %>% filter(year >= 2016),
               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/out_of_sample_data.feather")

write_feather(merged_data,
               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/full_data.feather")

in_sample <- merged_data %>% filter(year < 2016)
out_sample <- merged_data %>% filter(year >= 2016)
```

# Exploratory data analysis

Before defining any model, we need to explore the data to understand the distribution of our data and check for patterns. These patterns will be the leads to any advance machine learning model we could potentially implement. 

## Performance/risk analysis

### Autocorrelation - variable exploration

We have a lot of variables in our data set and as we can see below, a lot of them are either highly negatively correlated or highly positively correlated. 

```{r}
mydata.cor = cor(merged_data %>%
                   select_if(is.numeric) %>% 
                   na.omit(), method = c("spearman"))
palette = colorRampPalette(c("green", "white", "red")) (20)

heatmap(x = mydata.cor, col = palette, symm = TRUE)
```

We're trying to identify potential variables that are correlated with key metrics of the performance of a portfolio such as alpha, returns, etc. We'll cluster the data to identify which portfolios are part of a generally better performing group to avoid mixing effects. This will be done in another script. An alternative to this is a ranking system based on metrics of performance and risk as well as consistency throughout time which we present below.

### Ranking system

A first step in our analysis is identifying which portfolios have been better performing than others, taking into account the risk they took compared to the market state and their consistency across time. Let's start by ranking the portfolios according to five criteria:

- overperf: We take the average of the alpha of a portfolio across the whole period considered and see which have the highest.
- vol_alpha: We take the standard deviation of the alpha of each portfolio and see the highest.
- pondered_alpha: Using both average alpha and std of alpha, we combine both to take into account the consistency of the alpha of a portfolio.
- overallret: Consider the average return of the portfolio over the whole period.
- risk: We take the average risk of the portfolio considering the whole period.

We'll take the top portfolios for each and create groups for each to benchmark the same metrics but on non-aggregated time series data.

```{r}
ranking <- in_sample %>% 
  group_by(lpermno) %>% 
  summarize(overperf = mean(alpha),
            vol_alpha = sd(alpha),
            pondered_alpha = mean(alpha)/sd(alpha),
            risk = mean(b_mkt),
            overallret = mean(ret)) %>% 
  ungroup()

ranking

ranking_returns <- as.character(unique(ranking %>% 
  arrange(desc(overallret)) %>% 
  top_n(10))$lpermno)

ranking_alpha <- as.character(unique(ranking %>% 
  arrange(desc(overperf)) %>% 
  top_n(10))$lpermno)

ranking_palpha <- as.character(unique(ranking %>% 
  arrange(desc(pondered_alpha)) %>% 
  top_n(10))$lpermno)

ranking_vol <- as.character(unique(ranking %>% 
  arrange(desc(vol_alpha)) %>% 
  top_n(10))$lpermno)

ranking_risk <- as.character(unique(ranking %>% 
  arrange(desc(risk)) %>% 
  top_n(10))$lpermno)
```

Now that we have the portfolios for each ranking metric, we'll create groups to better identify them.

```{r}
ten_ret <- merged_data %>% 
  filter(lpermno %in% ranking_returns) %>% 
  mutate(group = "top ten returns")

ten_alp <- merged_data %>% 
  filter(lpermno %in% ranking_alpha) %>% 
  mutate(group = "top ten alpha")

ten_pal <- merged_data %>% 
  filter(lpermno %in% ranking_palpha) %>% 
  mutate(group = "top ten pondered alpha")

ten_vol <- merged_data %>% 
  filter(lpermno %in% ranking_vol) %>% 
  mutate(group = "top ten inconsistent")

ten_rsk <- merged_data %>% 
  filter(lpermno %in% ranking_risk) %>% 
  mutate(group = "top ten risk")

step1 <- ten_ret %>% 
  rbind(ten_alp) %>% 
  rbind(ten_pal) %>% 
  rbind(ten_vol) %>% 
  rbind(ten_rsk) %>% 
  mutate(i1 = row_number()) 

# remove duplicated in a seperate data frame
step2 <- step1 %>% 
  select(-group) %>% 
  distinct()

# filter the duplicated in the original data frame without dropping group variable
top_tier <- step1 %>% 
  filter(i1 %in% as.character(step2$i1)) %>% 
  select(-i1)

unique(step1$group)
```


Let's have an overview of their performance across time to better understand their positionning:

```{r}
par(mfrow=c(2,2))
pl_a <- top_tier %>% 
  mutate(lpermno = as.factor(lpermno),
         group = as.factor(group)) %>% 
  ggplot() +
  aes(x = date, y = alpha, color = group) +
  geom_point() +
  geom_line(aes(group=group)) +
  theme_bw() +
  ggtitle('Evolution of alpha over time') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
pl_b <- top_tier %>% 
  mutate(lpermno = as.factor(lpermno),
         group = as.factor(group)) %>% 
  ggplot() +
  aes(x = date, y = b_mkt, color = group) +
  geom_point() +
  geom_line(aes(group=group)) +
  theme_bw() +
  ggtitle('Evolution of beta over time') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
pl_r <- top_tier %>% 
  mutate(lpermno = as.factor(lpermno),
         group = as.factor(group)) %>% 
  ggplot() +
  aes(x = date, y = ret, color = group) +
  geom_point() +
  geom_line(aes(group=lpermno)) +
  theme_bw() +
  ggtitle('Evolution of returns over time')  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

ggplotly(p = pl_a, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")

ggplotly(p = pl_b, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")

ggplotly(p = pl_r, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")
```

```{r}
portfolios <- merged_data %>% 
  select(lpermno,ret,date) %>% 
  group_by(lpermno) %>% 
  spread(lpermno,ret)

portfolio_cors = cor(portfolios %>%
                   select(-date) %>% 
                   na.omit(), method = c("spearman"))
palette = colorRampPalette(c("green", "white", "red")) (20)

heatmap(x = portfolio_cors, col = palette, symm = TRUE)
```

```{r}
top_portfolios <- portfolios %>% 
  select(c(as.character(ranking$lpermno)))

top_portfolio_cors = cor(top_portfolios %>%
                   na.omit(), method = c("spearman"))
palette = colorRampPalette(c("green", "white", "red")) (20)

heatmap(x = top_portfolio_cors, col = palette, symm = TRUE)
```

