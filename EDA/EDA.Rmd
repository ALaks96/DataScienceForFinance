---
title: "EDA"
author: "Alexis Laks"
date: "4 décembre 2019"
output: 
  html_document:
    toc : true
---

```{r setup, include=FALSE}
# Markdown formatting
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

```{r}
#Lodaing libraries
library(tidyverse)
library(dummies)
library(corrplot)
library(kableExtra)
library(broom)
library(lubridate)
library(feather)
library(reshape2)
```

# Data formatting

We have three datasets which we can concatenate by date or by date and identifier. The data sets are the following:

- fundamentals.csv : 
- crsp_top1000.csv : information on stock returns and other stock characteristics for each identifier at a given date.
- FF.csv : information on characteristics of the global market for each month of years 1990 to 2019. 

From these data we obtain a significant amount of variables and rows. We need to find focus point to avoid a chaotic analysis. A first lead would be to check for performance of certain stocks across time for recent years. An indicator we can calculate and is missing from the data is the alpha or Jensen index which can be derived from the stock return formula:

r = Rf + beta * (Rm – Rf) + Alpha
<=> Alpha = r – Rf  – beta * (Rm – Rf)

where:

r = the security’s or stock’s return
Rf  = the risk-free rate of return
beta = systemic risk of a stock (the security’s or stock’s price volatility relative to the overall market)
Rm  = the market return

```{r}
# fundamentals data
ftls <- read.csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/fundamentals.csv", 
                 sep = "\t",
                 header = TRUE) %>% 
  mutate(day = substr(public_date,1,2),
         month = substr(public_date,3,5),
         year = substr(public_date,6,9)) %>% 
  mutate(month = case_when(month == "jan" ~ "1",
                           month == "feb" ~ "2",
                           month == "mar" ~ "3",
                           month == "apr" ~ "4",
                           month == "may" ~ "5",
                           month == "jun" ~ "6",
                           month == "jul" ~ "7",
                           month == "aug" ~ "8",
                           month == "sep" ~ "9",
                           month == "oct" ~ "10",
                           month == "nov" ~ "11",
                           month == "dec" ~ "12",
                           TRUE ~ "0")) %>% 
  select(-public_date)


# ff data

ff <- read.csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/FF.csv", 
                 sep = "\t",
                 header = TRUE) %>% 
  mutate(month = as.character(month),
         year = as.character(year))


# crsp top 100 data

crsp <- read.csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/crsp_top1000.csv", 
                 sep = "\t",
                 header = TRUE) %>% 
  mutate(month = substr(date,3,5),
         year = substr(date,6,9)) %>% 
  mutate(month = case_when(month == "jan" ~ "1",
                           month == "feb" ~ "2",
                           month == "mar" ~ "3",
                           month == "apr" ~ "4",
                           month == "may" ~ "5",
                           month == "jun" ~ "6",
                           month == "jul" ~ "7",
                           month == "aug" ~ "8",
                           month == "sep" ~ "9",
                           month == "oct" ~ "10",
                           month == "nov" ~ "11",
                           month == "dec" ~ "12",
                           TRUE ~ "0"))
```

We'll keep data only on stocks for which we have data on the whole period we consider. To do this, we check which stocks have the max number of dates in both fundamentals and crsp, we then retrieve the stock identifiers and keep only those in common. m

```{r}
valid_portfolios_ftls <- ftls %>% 
  group_by(lpermno) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n >= 348)

valid_portfolios_crsp <- crsp %>% 
  group_by(lpermno) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n >= 348)

valid_portfolios <- unique((valid_portfolios_crsp %>% 
  inner_join(valid_portfolios_ftls, 
             by = "lpermno"))$lpermno)
```

# ```{r}
# Just some checks to see how many portfolios we drop:
# length(valid_portfolios)
# 
# ftls %>%
#   group_by(lpermno) %>%
#   count() %>%
#   ungroup() %>%
#   group_by(n) %>%
#   count() %>%
#   filter(n <348) %>%
#   ungroup() %>%
#   summarize(sum(nn))
# ```

We drop a significant amount of stocks, however this can be backed by the fact that lack of information in for some periods would result in biased results and in turn a biased analysis. 

After filtering the stocks for which we have complete dara, we merge all the data in order to have all the information we potentially need going further in our analysis.

```{r}
crsp <- crsp %>% 
  filter(lpermno %in% valid_portfolios)

ftls <- ftls %>% 
  filter(lpermno %in% valid_portfolios)

merged_data <- crsp %>% 
    left_join(ftls,
              by = c("year","month","lpermno")) %>% 
    left_join(ff %>% 
                select(-date),
              by = c("year","month"))  %>% 
  drop_na(c(ret,rf,b_mkt,mktrf)) %>% 
  mutate(alpha = ret - rf - b_mkt * (mktrf - rf)) 

# Just creating a clean date format for future use in pyhton

merged_data <- merged_data %>% 
  mutate(date = with(merged_data, ymd(sprintf('%04s%02s%02s', year, month, day))))
```

Now that we've cleaned and formatted the data, we'll create two samples. The first one will be the in sample data which corresponds to stock data going from Jan 1990 to Dec 2015. The second one will be the out of sample data going from Jan 2016 till Dec 2019. We'll conduct our analysis and modelling on the in sample data and check our strategy's worth on the out of sample data.

```{r}
write_feather(merged_data %>% filter(year < 2016),
               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/in_sample_data.feather")

write_feather(merged_data %>% filter(year >= 2016),
               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/out_of_sample_data.feather")

write_feather(merged_data,
               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/full_data.feather")

in_sample <- merged_data %>% filter(year < 2016)
out_sample <- merged_data %>% filter(year >= 2016)
```

# Exploratory data analysis

Before defining any model, we need to explore the data to understand the distribution of our data and check for patterns. These patterns will be the leads to any advance machine learning model we could potentially implement. 

## Performance/risk analysis

### Autocorrelation - variable exploration

We have a lot of variables in our data set and as we can see below, a lot of them are either highly negatively correlated or highly positively correlated. 

```{r}
mydata.cor = cor(merged_data %>%
                   select_if(is.numeric) %>% 
                   na.omit(), method = c("spearman"))
palette = colorRampPalette(c("green", "white", "red")) (20)

heatmap(x = mydata.cor, col = palette, symm = TRUE)
```

We're trying to identify potential variables that are correlated with key metrics of the performance of a stock such as alpha, returns, etc. We'll cluster the data to identify which stocks are part of a generally better performing group to avoid mixing effects. This will be done in another script. An alternative to this is a ranking system based on metrics of performance and risk as well as consistency throughout time which we present below.

### Ranking system

A first step in our analysis is identifying which stocks have been better performing than others, taking into account the risk they took compared to the market state and their consistency across time. Let's start by ranking the portfolios according to five criteria:

- overperf: We take the average of the alpha of a stock across the whole period considered and see which have the highest.
- vol_alpha: We take the standard deviation of the alpha of each stock and see the highest.
- pondered_alpha: Using both average alpha and std of alpha, we combine both to take into account the consistency of the alpha of a stock.
- overallret: Consider the average return of the stock over the whole period.
- risk: We take the average risk of the stock considering the whole period.

We'll take the top stocks for each and create groups for each to benchmark the same metrics but on non-aggregated time series data.

```{r}
ranking <- out_sample %>% 
  group_by(lpermno) %>% 
  summarize(overperf = mean(alpha),
            vol_alpha = sd(alpha),
            pondered_alpha = mean(alpha)/sd(alpha),
            risk = mean(b_mkt),
            overallret = mean(ret)) %>% 
  ungroup()

ranking

ranking_returns <- as.character(unique(ranking %>% 
  arrange(desc(overallret)) %>% 
  top_n(10))$lpermno)

ranking_alpha <- as.character(unique(ranking %>% 
  arrange(desc(overperf)) %>% 
  top_n(10))$lpermno)

ranking_palpha <- as.character(unique(ranking %>% 
  arrange(desc(pondered_alpha)) %>% 
  top_n(10))$lpermno)

ranking_vol <- as.character(unique(ranking %>% 
  arrange(desc(vol_alpha)) %>% 
  top_n(10))$lpermno)

ranking_risk <- as.character(unique(ranking %>% 
  arrange(desc(risk)) %>% 
  top_n(10))$lpermno)
```

Now that we have the stocks for each ranking metric, we'll create groups to better identify them.

```{r}
ten_ret <- merged_data %>% 
  filter(lpermno %in% ranking_returns) %>% 
  mutate(group = "top ten returns")

ten_alp <- merged_data %>% 
  filter(lpermno %in% ranking_alpha) %>% 
  mutate(group = "top ten alpha")

ten_pal <- merged_data %>% 
  filter(lpermno %in% ranking_palpha) %>% 
  mutate(group = "top ten pondered alpha")

ten_vol <- merged_data %>% 
  filter(lpermno %in% ranking_vol) %>% 
  mutate(group = "top ten inconsistent")

ten_rsk <- merged_data %>% 
  filter(lpermno %in% ranking_risk) %>% 
  mutate(group = "top ten risk")

step1 <- ten_ret %>% 
  rbind(ten_alp) %>% 
  rbind(ten_pal) %>% 
  rbind(ten_vol) %>% 
  rbind(ten_rsk) %>%
  mutate(i1 = row_number())

# remove duplicated in a seperate data frame
step2 <- step1 %>%
  select(-group) %>%
  distinct()

# filter the duplicated in the original data frame without dropping group variable
top_tier <- step1 %>%
  filter(i1 %in% as.character(step2$i1)) %>%
  select(-i1)

#unique(step1$group)

#test <- unique(c(ranking_alpha,ranking_palpha,ranking_returns,ranking_risk,ranking_vol))
step1 %>% 
  select(lpermno) %>% 
  group_by(lpermno) %>% 
  count()
```

Let's have an overview of their performance across time to better understand their positionning:

```{r}
par(mfrow=c(2,2))
pl_a <- top_tier %>% 
  mutate(lpermno = as.factor(lpermno),
         group = as.factor(group)) %>% 
  ggplot() +
  aes(x = date, y = alpha, color = lpermno) +
  geom_point() +
  geom_line(aes(group=lpermno)) +
  theme_bw() +
  ggtitle('Evolution of alpha over time') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
pl_b <- top_tier %>% 
  mutate(lpermno = as.factor(lpermno),
         group = as.factor(group)) %>% 
  ggplot() +
  aes(x = date, y = b_mkt, color = lpermno) +
  geom_point() +
  geom_line() +
  theme_bw() +
  ggtitle('Evolution of beta over time') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
pl_r <- top_tier %>% 
  mutate(lpermno = as.factor(lpermno),
         group = as.factor(group)) %>% 
  ggplot() +
  aes(x = date, y = ret, color = lpermno) +
  geom_point() +
  geom_line(aes(group=lpermno)) +
  theme_bw() +
  ggtitle('Evolution of returns over time')  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

ggplotly(p = pl_a, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")

ggplotly(p = pl_b, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")

ggplotly(p = pl_r, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")
```

We can see that we consistently obtain the same stocks for all rankings. These may be stocks we coud choose for a portfolio.
However, we want our stocks to be the least correlated possible. We can check for that by looking at returns of all the stocks we have and filtering those which the correlation is below a certain threshold. 

# Uncorrelated stocks

In order to diversify risk, we'll search for the least correlated stocks in our data and check if they concur with the stocks that have the highest ranking stocks in terms of return, alpha, etc.

```{r}
# Transforming data to have stock identifiers as columns and rows as returns.
portfolios <- merged_data %>% 
  select(lpermno,ret,date) %>% 
  group_by(lpermno) %>% 
  spread(lpermno,ret)

# Creating correlation matrix and filter those with abs value below 0.05
cc <- cor(portfolios %>%
                   select(-date) %>% 
                   na.omit(), method = c("spearman"))

threshold <- 0.0005
cc0 <- cc
diag(cc0) <- 0
ok <- apply(abs(cc0) <= threshold, 1, any)
stonk_matrix <- cc[ok, ok]

# Creating a dataframe from the subset of ids for future use
stonk_dataframe <- subset(melt(cc), value < threshold & value > -threshold)

# Generating plot for correlation matrix
palette = colorRampPalette(c("green", "white", "red")) (20)
heatmap(x = stonk_matrix, col = palette, symm = TRUE)

# Retrieving id's satisfying our condition
stonks <- unique(c(stonk_dataframe$Var1, stonk_dataframe$Var2))

#filtering for those ids
uncorrelated_stonks <- merged_data %>% 
  filter(lpermno %in% as.character(stonks))

# plotting their returns over the whole period from the original data
pl_u <- uncorrelated_stonks %>% 
  mutate(lpermno = as.factor(lpermno)) %>% 
  ggplot() +
  aes(x = date, y = ret, color = lpermno) +
  geom_point() +
  geom_line(aes(group=lpermno)) +
  theme_bw() +
  ggtitle('Evolution of returns over time')  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

ggplotly(p = pl_u, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")
```

