---
title: "EDA"
author: "Alexis Laks"
date: "4 décembre 2019"
output: 
  html_document:
    toc : true
---

```{r setup, include=FALSE}
# Markdown formatting
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

```{r}
#Lodaing libraries
library(tidyverse)
library(dummies)
library(corrplot)
library(kableExtra)
library(broom)
library(lubridate)
library(feather)
```

# Data formatting

We have three datasets which we can concatenate by date or by date and identifier. The data sets are the following:

- fundamentals.csv : 
- crsp_top1000.csv : information on stock returns and other stock characteristics for each identifier at a given date.
- FF.csv : information on characteristics of the global market for each month of years 1990 to 2019. 

From these data we obtain a significant amount of variables and rows. We need to find focus point to avoid a chaotic analysis. A first lead would be to check for performance of certain portfolios across time for recent years. An indicator we can calculate and is missing from the data is the alpha or Jensen index which can be derived from the portfolio return formula:

r = Rf + beta * (Rm – Rf) + Alpha
<=> Alpha = r – Rf  – beta * (Rm – Rf)

where:

r = the security’s or portfolio’s return
Rf  = the risk-free rate of return
beta = systemic risk of a portfolio (the security’s or portfolio’s price volatility relative to the overall market)
Rm  = the market return

```{r}
# fundamentals data
ftls <- read.csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/fundamentals.csv", 
                 sep = "\t",
                 header = TRUE) %>% 
  mutate(day = substr(public_date,1,2),
         month = substr(public_date,3,5),
         year = substr(public_date,6,9)) %>% 
  mutate(month = case_when(month == "jan" ~ "1",
                           month == "feb" ~ "2",
                           month == "mar" ~ "3",
                           month == "apr" ~ "4",
                           month == "may" ~ "5",
                           month == "jun" ~ "6",
                           month == "jul" ~ "7",
                           month == "aug" ~ "8",
                           month == "sep" ~ "9",
                           month == "oct" ~ "10",
                           month == "nov" ~ "11",
                           month == "dec" ~ "12",
                           TRUE ~ "0")) %>% 
  select(-public_date)


# ff data

ff <- read.csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/FF.csv", 
                 sep = "\t",
                 header = TRUE) %>% 
  mutate(month = as.character(month),
         year = as.character(year))


# crsp top 100 data

crsp <- read.csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/crsp_top1000.csv", 
                 sep = "\t",
                 header = TRUE) %>% 
  mutate(month = substr(date,3,5),
         year = substr(date,6,9)) %>% 
  mutate(month = case_when(month == "jan" ~ "1",
                           month == "feb" ~ "2",
                           month == "mar" ~ "3",
                           month == "apr" ~ "4",
                           month == "may" ~ "5",
                           month == "jun" ~ "6",
                           month == "jul" ~ "7",
                           month == "aug" ~ "8",
                           month == "sep" ~ "9",
                           month == "oct" ~ "10",
                           month == "nov" ~ "11",
                           month == "dec" ~ "12",
                           TRUE ~ "0"))
```

We'll keep data only on portfolios for which we have data on the whole period we consider. To do this, we check which portfolios have the max number of dates in both fundamentals and crsp, we then retrieve the portfolio identifiers and keep only those in common. m

```{r}
valid_portfolios_ftls <- ftls %>% 
  group_by(lpermno) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n >= 348)

valid_portfolios_crsp <- crsp %>% 
  group_by(lpermno) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n >= 348)

valid_portfolios <- unique((valid_portfolios_crsp %>% 
  inner_join(valid_portfolios_ftls, 
             by = "lpermno"))$lpermno)
```

# ```{r}
# Just some checks to see how many portfolios we drop:
# length(valid_portfolios)
# 
# ftls %>%
#   group_by(lpermno) %>%
#   count() %>%
#   ungroup() %>%
#   group_by(n) %>%
#   count() %>%
#   filter(n <348) %>%
#   ungroup() %>%
#   summarize(sum(nn))
# ```

We drop a significant amount of portfolios, however this can be backed by the fact that lack of information in for some periods would result in biased results and in turn a biased analysis. 

After filtering the portfolios for which we have complete dara, we merge all the data in order to have all the information we potentially need going further in our analysis.

```{r}
crsp <- crsp %>% 
  filter(lpermno %in% valid_portfolios)

ftls <- ftls %>% 
  filter(lpermno %in% valid_portfolios)

merged_data <- crsp %>% 
    left_join(ftls,
              by = c("year","month","lpermno")) %>% 
    left_join(ff %>% 
                select(-date),
              by = c("year","month"))  %>% 
  drop_na(c(ret,rf,b_mkt,mktrf)) %>% 
  mutate(alpha = ret - rf - b_mkt * (mktrf - rf)) 

# Just creating a clean date format for future use in pyhton

merged_data <- merged_data %>% 
  mutate(date = with(merged_data, ymd(sprintf('%04s%02s%02s', year, month, day))))
```

Now that we've cleaned and formatted the data, we'll create two samples. The first one will be the in sample data which corresponds to portfolio data going from Jan 1990 to Dec 2015. The second one will be the out of sample data going from Jan 2016 till Dec 2019. We'll conduct our analysis and modelling on the in sample data and check our strategy's worth on the out of sample data.

```{r}
write_feather(merged_data %>% filter(year < 2016),
               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/in_sample_data.feather")

write_feather(merged_data %>% filter(year >= 2016),
               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/out_of_sample_data.feather")

write_feather(merged_data,
               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/full_data.feather")

in_sample <- merged_data %>% filter(year < 2016)
out_sample <- merged_data %>% filter(year >= 2016)
```

# Exploratory data analysis

Before defining any model, we need to explore the data to understand the distribution of our data and check for patterns. These patterns will be the leads to any advance machine learning model we could potentially implement. 

## Performance/risk analysis

A first step in our analysis is identifying which portfolios have been better performing than others, taking into account the risk they took compared to the market state and their consistency across time. Let's start by ranking the portfolios according to their average alpha over the past 4 years:

```{r}
ranking <- in_sample %>% 
  group_by(lpermno) %>% 
  summarize(overperf = mean(alpha),
            vol_alpha = sd(alpha),
            pondered_alpha = mean(alpha)/sd(alpha),
            risk = mean(b_mkt),
            overallret = mean(ret)) %>% 
  ungroup() %>% 
  arrange(desc(overallret)) %>% 
  top_n(10)
ranking
```

Let's have an overview of their performance across time to better understand their positionning:

```{r}
par(mfrow=c(2,2))
pl_a <- merged_data %>% 
  filter(lpermno %in% c(ranking$lpermno)) %>% 
  mutate(lpermno = as.factor(lpermno)) %>% 
  ggplot() +
  aes(x = date, y = alpha, color = lpermno) +
  geom_point() +
  geom_line(aes(group=lpermno)) +
  theme_bw() +
  ggtitle('Evolution of alpha over time') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
pl_b <- merged_data %>% 
  filter(lpermno %in% c(ranking$lpermno)) %>% 
  mutate(lpermno = as.factor(lpermno)) %>% 
  ggplot() +
  aes(x = date, y = b_mkt, color = lpermno) +
  geom_point() +
  geom_line(aes(group=lpermno)) +
  theme_bw() +
  ggtitle('Evolution of beta over time') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
pl_r <- merged_data %>% 
  filter(lpermno %in% c(ranking$lpermno)) %>% 
  mutate(lpermno = as.factor(lpermno)) %>% 
  ggplot() +
  aes(x = date, y = ret, color = lpermno) +
  geom_point() +
  geom_line(aes(group=lpermno)) +
  theme_bw() +
  ggtitle('Evolution of returns over time')  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

ggplotly(p = pl_a, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")

ggplotly(p = pl_b, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")

ggplotly(p = pl_r, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")
```

```{r}
# mydata.cor <- cor(merged_data %>% 
#                     select_if(is.numeric))
mydata.cor = cor(merged_data %>%
                   select_if(is.numeric) %>% 
                   na.omit(), method = c("spearman"))
palette = colorRampPalette(c("green", "white", "red")) (20)

heatmap(x = mydata.cor, col = palette, symm = TRUE)

```

