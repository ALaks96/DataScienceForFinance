---
title: "EDA"
author: "Alexis Laks"
date: "4 décembre 2019"
output: 
  html_document:
    toc : true
---

```{r setup, include=FALSE}
# Markdown formatting
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

```{r}
#Lodaing libraries
library(tidyverse)
library(dummies)
library(corrplot)
library(kableExtra)
library(broom)
library(lubridate)
library(feather)
library(reshape2)
library(tibbletime)
library(plotly)
#install.packages("tidyverse")
#install.packages("tibbletime")
```

# Data formatting

We have three datasets which we can concatenate by date or by date and identifier. The data sets are the following:

- fundamentals.csv : 
- crsp_top1000.csv : information on stock returns and other stock characteristics for each identifier at a given date.
- FF.csv : information on characteristics of the global market for each month of years 1990 to 2019. 

From these data we obtain a significant amount of variables and rows. We need to find focus point to avoid a chaotic analysis. A first lead would be to check for performance of certain stocks across time for recent years. An indicator we can calculate and is missing from the data is the alpha or Jensen index which can be derived from the stock return formula:

r = Rf + beta * (Rm – Rf) + Alpha
<=> Alpha = r – Rf  – beta * (Rm – Rf)

where:

r = the security’s or stock’s return
Rf  = the risk-free rate of return
beta = systemic risk of a stock (the security’s or stock’s price volatility relative to the overall market)
Rm  = the market return

```{r}
# fundamentals data
ftls <- read.csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/fundamentals.csv", 
                 sep = "\t",
                 header = TRUE) %>% 
  mutate(day = substr(public_date,1,2),
         month = substr(public_date,3,5),
         year = substr(public_date,6,9)) %>% 
  mutate(month = case_when(month == "jan" ~ "1",
                           month == "feb" ~ "2",
                           month == "mar" ~ "3",
                           month == "apr" ~ "4",
                           month == "may" ~ "5",
                           month == "jun" ~ "6",
                           month == "jul" ~ "7",
                           month == "aug" ~ "8",
                           month == "sep" ~ "9",
                           month == "oct" ~ "10",
                           month == "nov" ~ "11",
                           month == "dec" ~ "12",
                           TRUE ~ "0")) %>% 
  select(-public_date)


# ff data

ff <- read.csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/FF.csv", 
                 sep = "\t",
                 header = TRUE) %>% 
  mutate(month = as.character(month),
         year = as.character(year))


# crsp top 100 data

crsp <- read.csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/crsp_top1000.csv", 
                 sep = "\t",
                 header = TRUE) %>% 
  mutate(month = substr(date,3,5),
         year = substr(date,6,9)) %>% 
  mutate(month = case_when(month == "jan" ~ "1",
                           month == "feb" ~ "2",
                           month == "mar" ~ "3",
                           month == "apr" ~ "4",
                           month == "may" ~ "5",
                           month == "jun" ~ "6",
                           month == "jul" ~ "7",
                           month == "aug" ~ "8",
                           month == "sep" ~ "9",
                           month == "oct" ~ "10",
                           month == "nov" ~ "11",
                           month == "dec" ~ "12",
                           TRUE ~ "0"))
```

We'll keep data only on stocks for which we have data on the whole period we consider. To do this, we check which stocks have the max number of dates in both fundamentals and crsp, we then retrieve the stock identifiers and keep only those in common. m

```{r}
valid_portfolios_ftls <- ftls %>% 
  group_by(lpermno) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n >= 348)

valid_portfolios_crsp <- crsp %>% 
  group_by(lpermno) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n >= 348)

valid_portfolios <- unique((valid_portfolios_crsp %>% 
  inner_join(valid_portfolios_ftls, 
             by = "lpermno"))$lpermno)
```

# ```{r}
# Just some checks to see how many portfolios we drop:
# length(valid_portfolios)
# 
# ftls %>%
#   group_by(lpermno) %>%
#   count() %>%
#   ungroup() %>%
#   group_by(n) %>%
#   count() %>%
#   filter(n <348) %>%
#   ungroup() %>%
#   summarize(sum(nn))
# ```

We drop a significant amount of stocks, however this can be backed by the fact that lack of information in for some periods would result in biased results and in turn a biased analysis. 

After filtering the stocks for which we have complete dara, we merge all the data in order to have all the information we potentially need going further in our analysis.

```{r}
crsp <- crsp %>% 
  filter(lpermno %in% valid_portfolios)

ftls <- ftls %>% 
  filter(lpermno %in% valid_portfolios)

merged_data <- crsp %>% 
    left_join(ftls,
              by = c("year","month","lpermno")) %>% 
    left_join(ff %>% 
                select(-date),
              by = c("year","month"))  %>% 
  drop_na(c(ret,rf,b_mkt,mktrf)) %>% 
  mutate(alpha = ret - rf - b_mkt * (mktrf - rf)) 

# Just creating a clean date format for future use in pyhton

merged_data <- merged_data %>% 
  mutate(date = with(merged_data, ymd(sprintf('%04s%02s%02s', year, month, day))))
```

Now that we've cleaned and formatted the data, we'll create two samples. The first one will be the in sample data which corresponds to stock data going from Jan 1990 to Dec 2015. The second one will be the out of sample data going from Jan 2016 till Dec 2019. We'll conduct our analysis and modelling on the in sample data and check our strategy's worth on the out of sample data.

```{r}
write_feather(merged_data %>% filter(year < 2016),
               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/in_sample_data.feather")

write_feather(merged_data %>% filter(year >= 2016),
               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/out_of_sample_data.feather")

write_feather(merged_data,
               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/full_data.feather")


merged_data <- read_feather("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/full_data.feather")
in_sample <- merged_data %>% filter(year < 2016)
out_sample <- merged_data %>% filter(year >= 2016)
```

# Exploratory data analysis

Before defining any model, we need to explore the data to understand the distribution of our data and check for patterns. These patterns will be the leads to any advance machine learning model we could potentially implement. 

## Performance/risk analysis

### Autocorrelation - variable exploration

We have a lot of variables in our data set and as we can see below, a lot of them are either highly negatively correlated or highly positively correlated. 

```{r}
mydata.cor = cor(merged_data %>%
                   select_if(is.numeric) %>% 
                   na.omit(), method = c("spearman"))
palette = colorRampPalette(c("green", "white", "red")) (20)

heatmap(x = mydata.cor, col = palette, symm = TRUE)
```

We're trying to identify potential variables that are correlated with key metrics of the performance of a stock such as alpha, returns, etc. We'll cluster the data to identify which stocks are part of a generally better performing group to avoid mixing effects. This will be done in another script. An alternative to this is a ranking system based on metrics of performance and risk as well as consistency throughout time which we present below.

### Ranking system

A first step in our analysis is identifying which stocks have been better performing than others, taking into account their risk compared to the market state and their consistency across time. Let's start by ranking the portfolios according to five criteria:

- overperf: We take the average of the alpha of a stock across the whole period considered and see which have the highest.
- vol_alpha: We take the standard deviation of the alpha of each stock and see the highest.
- pondered_alpha: Using both average alpha and std of alpha, we combine both to take into account the consistency of the alpha of a stock.
- overallret: Consider the average return of the stock over the whole period.
- risk: We take the average risk of the stock considering the whole period.

```{r}
ranking <- merged_data %>% 
  group_by(lpermno) %>% 
  summarize(overperf = mean(alpha),
            vol_alpha = sd(alpha),
            pondered_alpha = mean(alpha)/sd(alpha),
            risk = mean(b_mkt),
            overallret = mean(ret)) %>% 
  ungroup()
ranking
```

We'll start by looking at the top ten best and worst stocks in terms of the above criteria to get a better understanding of how our sotcks are distributed.

#### Top ten best

We'll take the top stocks for each and create groups for each to benchmark the same metrics but on non-aggregated time series data.

```{r}
ranking_returns <- as.character(unique(ranking %>% 
  arrange(desc(overallret)) %>% 
  top_n(10))$lpermno)

ranking_alpha <- as.character(unique(ranking %>% 
  arrange(desc(overperf)) %>% 
  top_n(10))$lpermno)

ranking_palpha <- as.character(unique(ranking %>% 
  arrange(desc(pondered_alpha)) %>% 
  top_n(10))$lpermno)

ranking_vol <- as.character(unique(ranking %>% 
  arrange(desc(vol_alpha)) %>% 
  top_n(10))$lpermno)

ranking_risk <- as.character(unique(ranking %>% 
  arrange(desc(risk)) %>% 
  top_n(10))$lpermno)
```

Now that we have the stocks for each ranking metric, we'll create groups to better identify them.

```{r}
ten_ret <- merged_data %>% 
  filter(lpermno %in% ranking_returns) %>% 
  mutate(group = "top ten returns")

ten_alp <- merged_data %>% 
  filter(lpermno %in% ranking_alpha) %>% 
  mutate(group = "top ten alpha")

ten_pal <- merged_data %>% 
  filter(lpermno %in% ranking_palpha) %>% 
  mutate(group = "top ten pondered alpha")

ten_vol <- merged_data %>% 
  filter(lpermno %in% ranking_vol) %>% 
  mutate(group = "top ten inconsistent")

ten_rsk <- merged_data %>% 
  filter(lpermno %in% ranking_risk) %>% 
  mutate(group = "top ten risk")

step1 <- ten_ret %>% 
  rbind(ten_alp) %>% 
  rbind(ten_pal) %>% 
  rbind(ten_vol) %>% 
  rbind(ten_rsk) %>%
  mutate(i1 = row_number())

# remove duplicated in a seperate data frame
step2 <- step1 %>%
  select(-group) %>%
  distinct()

# filter the duplicated in the original data frame without dropping group variable
top_tier <- step1 %>%
  filter(i1 %in% as.character(step2$i1)) %>%
  select(-i1)

#unique(step1$group)

#test <- unique(c(ranking_alpha,ranking_palpha,ranking_returns,ranking_risk,ranking_vol))
step1 %>% 
  select(lpermno) %>% 
  group_by(lpermno) %>% 
  count()
```

Let's have an overview of their performance across time to better understand their positionning:

```{r}
par(mfrow=c(2,2))
pl_a <- top_tier %>% 
  mutate(lpermno = as.factor(lpermno),
         group = as.factor(group)) %>% 
  ggplot() +
  aes(x = date, y = alpha, color = lpermno) +
  geom_point() +
  geom_line(aes(group=lpermno)) +
  theme_bw() +
  ggtitle('Evolution of alpha over time') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
pl_b <- top_tier %>% 
  mutate(lpermno = as.factor(lpermno),
         group = as.factor(group)) %>% 
  ggplot() +
  aes(x = date, y = b_mkt, color = lpermno) +
  geom_point() +
  geom_line() +
  theme_bw() +
  ggtitle('Evolution of beta over time') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
pl_r <- top_tier %>% 
  mutate(lpermno = as.factor(lpermno),
         group = as.factor(group)) %>% 
  ggplot() +
  aes(x = date, y = ret, color = lpermno) +
  geom_point() +
  geom_line(aes(group=lpermno)) +
  theme_bw() +
  ggtitle('Evolution of returns over time')  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

ggplotly(p = pl_a, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")

ggplotly(p = pl_b, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")

ggplotly(p = pl_r, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")
```

We can see that we consistently obtain the same stocks for all rankings which isn't what we're aiming for. Nonetheless, we can see that across time volatility has clearly decreased for the lump of stocks considered here. 

#### Top ten worst 

Now we should look at the worst performing stocks according to the same criteria:

```{r}
ranking_returns_w <- as.character(unique(ranking %>% 
  arrange(desc(overallret)) %>% 
  top_n(10))$lpermno)

ranking_alpha_w <- as.character(unique(ranking %>% 
  arrange(overperf) %>% 
  top_n(10))$lpermno)

ranking_palpha_w <- as.character(unique(ranking %>% 
  arrange(pondered_alpha) %>% 
  top_n(10))$lpermno)

ranking_vol_w <- as.character(unique(ranking %>% 
  arrange(vol_alpha) %>% 
  top_n(10))$lpermno)

ranking_risk_w <- as.character(unique(ranking %>% 
  arrange(risk) %>% 
  top_n(10))$lpermno)


ranking %>% 
  ggplot() +
  aes(x = overperf) +
  geom_histogram()

ranking <- merged_data %>% 
  group_by(siccd) %>% 
  summarize(overperf = mean(alpha),
            vol_alpha = sd(alpha),
            pondered_alpha = mean(alpha)/sd(alpha),
            risk = mean(b_mkt),
            overallret = mean(ret)) %>% 
  ungroup() %>% 
  arrange(desc(overallret))

ranking <- ranking[1:10,]
```

```{r}
ten_ret_w <- merged_data %>% 
  filter(lpermno %in% ranking_returns) %>% 
  mutate(group = "top ten worst returns")

ten_alp_w <- merged_data %>% 
  filter(lpermno %in% ranking_alpha) %>% 
  mutate(group = "top ten worst alpha")

ten_pal_w <- merged_data %>% 
  filter(lpermno %in% ranking_palpha) %>% 
  mutate(group = "top ten worst pondered alpha")

ten_vol_w <- merged_data %>% 
  filter(lpermno %in% ranking_vol) %>% 
  mutate(group = "top ten worst inconsistent")

ten_rsk_w <- merged_data %>% 
  filter(lpermno %in% ranking_risk) %>% 
  mutate(group = "top ten worst risk")

step1w <- ten_ret_w %>% 
  rbind(ten_alp_w) %>% 
  rbind(ten_pal_w) %>% 
  rbind(ten_vol_w) %>% 
  rbind(ten_rsk_w) %>%
  mutate(i1 = row_number())

# remove duplicated in a seperate data frame
step2w <- step1w %>%
  select(-group) %>%
  distinct()

# filter the duplicated in the original data frame without dropping group variable
top_tier_w <- step1w %>%
  filter(i1 %in% as.character(step2w$i1)) %>%
  select(-i1)

#unique(step1$group)

#test <- unique(c(ranking_alpha,ranking_palpha,ranking_returns,ranking_risk,ranking_vol))
# step1 %>% 
#   select(lpermno) %>% 
#   group_by(lpermno) %>% 
#   count()
```

Now to visually represent them:

```{r}
par(mfrow=c(2,2))
pl_a_w <- top_tier %>% 
  mutate(lpermno = as.factor(lpermno),
         group = as.factor(group)) %>% 
  ggplot() +
  aes(x = date, y = alpha, color = lpermno) +
  geom_point() +
  geom_line(aes(group=lpermno)) +
  theme_bw() +
  ggtitle('Evolution of alpha over time') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
pl_b_w <- top_tier %>% 
  mutate(lpermno = as.factor(lpermno),
         group = as.factor(group)) %>% 
  ggplot() +
  aes(x = date, y = b_mkt, color = lpermno) +
  geom_point() +
  geom_line() +
  theme_bw() +
  ggtitle('Evolution of beta over time') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
pl_r_w <- top_tier %>% 
  mutate(lpermno = as.factor(lpermno),
         group = as.factor(group)) %>% 
  ggplot() +
  aes(x = date, y = ret, color = lpermno) +
  geom_point() +
  geom_line(aes(group=lpermno)) +
  theme_bw() +
  ggtitle('Evolution of returns over time')  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

ggplotly(p = pl_a_w, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")

ggplotly(p = pl_b_w, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")

ggplotly(p = pl_r_w, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")
```


#### Uncorrelated stocks

However, we want our stocks to be the least correlated possible. We can check for that by looking at returns of all the stocks we have and filtering those which the correlation is below a certain threshold. 
In order to diversify risk, we'll search for the least correlated stocks in our data and check if they concur with the stocks that have the highest ranking stocks in terms of return, alpha, etc.

```{r}
# Transforming data to have stock identifiers as columns and rows as returns.
portfolios <- merged_data %>% 
  select(lpermno,ret,date) %>% 
  group_by(lpermno) %>% 
  spread(lpermno,ret)

# Creating correlation matrix and filter those with abs value below 0.05
cc <- cor(portfolios %>%
                   select(-date) %>% 
                   na.omit(),
          method = c("spearman"))

threshold <- 0.0005
cc0 <- cc
diag(cc0) <- 0
ok <- apply(abs(cc0) <= threshold, 1, any)
stonk_matrix <- cc[ok, ok]
cc[abs(cc) < 0.005] <- NA

# Creating a dataframe from the subset of ids for future use
stonk_dataframe <- subset(melt(cc), value < threshold & value > -threshold)

# Generating plot for correlation matrix
palette = colorRampPalette(c("green", "white", "red")) (20)
heatmap(x = stonk_matrix, col = palette, symm = TRUE)

# Retrieving id's satisfying our condition
stonks <- unique(c(stonk_dataframe$Var1, stonk_dataframe$Var2))

#filtering for those ids
uncorrelated_stonks <- merged_data %>% 
  filter(lpermno %in% as.character(stonks))

# plotting their returns over the whole period from the original data
pl_u <- uncorrelated_stonks %>% 
  mutate(lpermno = as.factor(lpermno)) %>% 
  ggplot() +
  aes(x = date, y = ret, color = lpermno) +
  geom_point() +
  geom_line(aes(group=lpermno)) +
  theme_bw() +
  ggtitle('Evolution of returns over time')  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

ggplotly(p = pl_u, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")
```

#### Industry selection

Another approach to get better insights onwhich stocks would be most interesting is to focus on specific industries. To do this, we start by computing the rolling average of returns of each stock with a window of 12 months. We then compute the average per industry of these rolling averages for each month/year and keep only the top 10 industries according to that metric for each date. 

```{r}
rolling_mean <- rollify(mean, window = 12)
  
top_indus <- merged_data %>% 
  group_by(lpermno) %>% 
  mutate(mean_12 = rolling_mean(ret)) %>% 
  ungroup() %>% 
  group_by(siccd,year,month) %>% 
  summarize(av_ret = mean(mean_12)) %>%
  ungroup() %>% 
  group_by(year,month) %>% 
  top_n(10,av_ret) %>%
  ungroup() %>% 
  arrange(as.numeric(year),as.numeric(month)) %>% 
  mutate(date = with(top_indus, sprintf("%s-%02s", year, month))) 

spl_indus <- top_indus %>% 
  ggplot() +
  aes(x = date, y = siccd, color = av_ret) %>% 
  geom_point() +
  theme(axis.text.x = element_text(angle = 45))

ggplotly(p = pl_indus, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")

top_10_best <- (top_indus %>% 
  group_by(siccd) %>% 
  count() %>% 
  ungroup() %>% 
  arrange(desc(n)))[1:10,]$siccd

# for Rick #
# rolling_data <- merged_data %>% 
#   group_by(lpermno) %>% 
#   mutate(rolling_ret_12 = rolling_mean(ret)) %>% 
#   ungroup()
# write_feather(rolling_data,
#               "~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/rolling_data.feather")
```

We then go through this same process only to select the top 10 worst industries in terms of returns using the same methodology

```{r}
rolling_mean <- rollify(mean, window = 12)
  
worst_indus <- merged_data %>% 
  group_by(lpermno) %>% 
  mutate(mean_12 = rolling_mean(ret)) %>% 
  ungroup() %>% 
  group_by(siccd,year,month) %>% 
  summarize(av_ret = mean(mean_12)) %>%
  ungroup() %>% 
  group_by(year,month) %>% 
  top_n(-10,av_ret) %>%
  ungroup() %>% 
  arrange(as.numeric(year),as.numeric(month)) %>% 
  mutate(date = with(top_indus, sprintf("%s-%02s", year, month))) 

pl_indus_w <- worst_indus %>% 
  ggplot() +
  aes(x = date, y = siccd, color = av_ret) %>% 
  geom_point() +
  theme(axis.text.x = element_text(angle = 45))

ggplotly(p = pl_indus_w, width = NULL, height = NULL,
  tooltip = "all", dynamicTicks = FALSE, layerData = 1,
  originalData = TRUE, source = "A")

top_10_worst <- (worst_indus %>% 
  group_by(siccd) %>% 
  count() %>% 
  ungroup() %>% 
  arrange(desc(n)))[1:10,]$siccd
```

```{r}
data1 <- read_csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/please.csv")

best_10 <- unique(((data1 %>% 
  filter(year == "2016" & month == "1") %>% 
  group_by(siccd) %>% 
  summarize(roll_av_av = mean(rolling_ret_12)) %>% 
  ungroup() %>% 
  arrange(desc(roll_av_av))))[1:10,]$siccd)

worst_10 <- unique(((data1 %>% 
  filter(year == "2016" & month == "1") %>% 
  group_by(siccd) %>% 
  summarize(roll_av_av = mean(rolling_ret_12)) %>% 
  ungroup() %>% 
  arrange(roll_av_av)))[1:10,]$siccd)
valid_indus <- c(best_10, worst_10)
```

```{r}
# After checking qualitatively which variables made sense in a hypothetical regression to maximize alpha...
data <- read_csv("~/Desktop/HEC Paris/Cours/DataScienceFinance/DataScienceForFinance/data/Data/regression_ready.csv")

in_sample <- data %>% 
  select(c(capei,bm,evm,pe_op_basic,pe_exi,pe_inc,ps,pcf,dpr,npm,opmbd,opmad,gpm,ptpm,cfm,roa,roe,roce,efftax,aftret_eq,aftret_invcapx,aftret_equity,pretret_noa,pretret_earnat,gprof,equity_invcap,debt_invcap,totdebt_invcap,capital_ratio,int_debt,int_totdebt,cash_lt,rect_act,debt_at,debt_ebitda,short_debt,curr_debt,lt_debt,profit_lct,ocf_lct,cash_debt,fcf_ocf,lt_ppent,dltt_be,debt_assets,debt_capital,de_ratio,intcov,intcov_ratio,cash_ratio,quick_ratio,curr_ratio,at_turn,rect_turn,sale_invcap,sale_equity,sale_nwc,rd_sale,adv_sale,staff_sale,accrual,ptb,peg_trailing,peg_1yrforward,peg_ltgforward,alpha_next,siccd,year)) %>% 
  filter(year < 2016) %>% 
  select(-year)
out_sample <- data %>% 
  select(c(capei,bm,evm,pe_op_basic,pe_exi,pe_inc,ps,pcf,dpr,npm,opmbd,opmad,gpm,ptpm,cfm,roa,roe,roce,efftax,aftret_eq,aftret_invcapx,aftret_equity,pretret_noa,pretret_earnat,gprof,equity_invcap,debt_invcap,totdebt_invcap,capital_ratio,int_debt,int_totdebt,cash_lt,rect_act,debt_at,debt_ebitda,short_debt,curr_debt,lt_debt,profit_lct,ocf_lct,cash_debt,fcf_ocf,lt_ppent,dltt_be,debt_assets,debt_capital,de_ratio,intcov,intcov_ratio,cash_ratio,quick_ratio,curr_ratio,at_turn,rect_turn,sale_invcap,sale_equity,sale_nwc,rd_sale,adv_sale,staff_sale,accrual,ptb,peg_trailing,peg_1yrforward,peg_ltgforward,alpha_next,siccd,year)) %>% 
  filter(year >= 2016) %>% 
  select(-year)

# We then initialize a null model, a full model and apply a stepwise selection method to isolate for each industry which accounting features could be considered for optimizing a portfolio. We loop through every industry to isolate for market effects. 
n <- 0
acc_gen <- 0

for (indus in valid_indus){
  print(indus)
  temp_out <- out_sample %>%
                   filter(siccd == indus) %>% 
                   select(-siccd)
  temp2 <- in_sample %>%
                   filter(siccd == indus) %>% 
                   select(-siccd)
  if (indus == "6211"){
    temp2 <- temp2 %>% 
      select(-int_totdebt)
  }
  temp3 <- temp2[, colSums(is.na(temp2)) != nrow(temp2)]
  
  null_model <- lm(alpha_next ~ 1 , data = temp3 %>% na.omit())
  full_model <- lm(alpha_next ~ . , data = temp3 %>% na.omit()) 
  stepwise_selec <- step(null_model,
                       direction = "both",
                       scope = list(upper=full_model,lower = null_model),
                       trace=0)
  print(summary(stepwise_selec))
  new_alpha <- predict(stepwise_selec, 
          temp_out
          #%>% select(names(stepwise_selec$coefficients)[2:length(stepwise_selec$coefficients)])
          )
  temp4 <- temp_out %>% 
    cbind(new_alpha) %>% 
    mutate(acc = (new_alpha - alpha_next)) %>% 
    summarize(mean(acc))
  
  if (is.na(temp4$`mean(acc)`) == FALSE) {
    n <- n + 1
    acc_gen <- acc_gen + temp4$`mean(acc)`
    
  }
}

acc_gen/n
temp4$`mean(acc)`
```

Across each industry we compute the mean difference in 